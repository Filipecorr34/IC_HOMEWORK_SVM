{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "145451bb",
      "metadata": {
        "id": "145451bb"
      },
      "source": [
        "# IC-UFPA-HOMEWORK sobre SVMs\n",
        "\n",
        "Filipe Corrêa da Silva - 202006840020"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "451ba1af",
      "metadata": {
        "id": "451ba1af"
      },
      "source": [
        "    1 - A figura abaixo mostra os exemplos de treino e as regiões de decisão de uma SVM (ou SVC, de “support vector classifier”) para um problema binário (duas classes onde o vetor x de “features” de entrada tem dois parâmetros (weight e length). Os pontos em azul e vermelho, representam os exemplos de cada classe. As regiões em azul e vermelho representam as regiões nas quais o classificador irá prever uma classe azul e vermelha, respectivamente. Olhando para as figuras, informe:\n",
        "    \n",
        "    a) Quantos erros cada SVM possui no conjunto de treino?\n",
        "\n",
        "LinearSVC (linear kernel) -> 2 erros\n",
        "\n",
        "SVC with linear kernel -> 1 erro\n",
        "\n",
        "SVC with RBF kernel -> 1 erro\n",
        "\n",
        "SVC with polynomial (degree 3) kernel -> 0 erros\n",
        "\n",
        "    b) Qual a que lhe parece ser a melhor SVM? Por que?\n",
        "\n",
        "A melhor SVM me parece ser a última (SVC with polynomial (degree 3) kernel),  pois ela não apresenta erros no seu conjunto de treino.    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c360637c",
      "metadata": {
        "id": "c360637c"
      },
      "source": [
        "    2 - A regularização é uma importante ferramenta para controlar a “complexidade do modelo” durante a fase de seu projeto. No projeto de uma SVM, o hiperparâmetro C é usado para efetuar a regularização do modelo na etapa de seleção do modelo, ou seja, quando os hiperparâmetros são variados e observamos o impacto através do erro de classificação no conjunto de validação. Suponha que você se encontra em uma situação onde está descontente com o atual resultado de sua SVM e observa que o número de vetores de suporte está relativamente pequeno. Você quer potencialmente aumentar o número de vetores de suporte, fazendo com que sua nova SVM seja um modelo mais “complexo”. Para isso, você deve aumentar ou diminuir o parâmetro “C” da classe SVC do scikit-learn?    \n",
        "\n",
        "Com o parâmetro C aumentado, a SVM prioriza a classificação correta dos dados de treinamento, mesmo que a margem seja menor. Isso resulta em mais vetores de suporte, pois o modelo se ajusta mais aos dados, incluindo pontos próximos ao hiperplano. Logo, se o número de vetores de suporte está muito pequeno e se deseja aumentar a complexidade do modelo, aumentar C fará com que a SVM seja mais flexível, incluindo mais vetores de suporte e reduzindo a margem."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3504bbbd",
      "metadata": {
        "id": "3504bbbd"
      },
      "source": [
        "    3 - Um classificador usa vetores de entrada com dimensão K=5 “features”. Após treinar uma SVM linear, o número de vetores de suporte foi de 450 exemplos. Neste caso, calcular o kernel linear corresponde a um produto interno entre dois vetores de dimensão K=5, cada, o que requer K multiplicações e K-1 adições. Estime o fator F = Coriginal / Cperceptron de redução do custo computacional da etapa de teste ao converter esta SVM linear para um perceptron. Assuma que os custos Coriginal Cperceptron correspondem ao número de multiplicações e adições usando-se respectivamente, a SVM original com os 450 vetores de suporte e após sua conversão para perceptron.\n",
        "\n",
        "Cálculo do Custo da SVM Original:\n",
        "Em uma SVM linear com 450 vetores de suporte (SV), a classificação de uma nova entrada x é dada por:\n",
        "\n",
        "$$\n",
        "f(\\mathbf{x}) = \\text{sgn}\\left( \\sum_{i=1}^{450} \\alpha_i y_i \\langle \\mathbf{x}_i, \\mathbf{x} \\rangle + b \\right)\n",
        "$$\n",
        "\n",
        "Cálculo do Produto Interno (<xi, x>):\n",
        "Para cada vetor de suporte xi, o produto interno com x requer:\n",
        "- **5 multiplicações** (uma para cada dimensão),\n",
        "- **4 adições** (para somar os resultados das multiplicações).\n",
        "\n",
        "Como há **450 vetores de suporte**, o custo total para calcular todos os produtos internos é:\n",
        "\n",
        "$$\n",
        "450 \\times (5 \\text{ multiplicações} + 4 \\text{ adições}) = 450 \\times 9 = 4050 \\text{ operações}.\n",
        "$$\n",
        "\n",
        "Cálculo da Soma Ponderada e Bias:\n",
        "1. Multiplicar cada $ <xi, x> $ por $ \\alpha_i y_i $ (**450 multiplicações**),\n",
        "2. Somar todos os termos (**449 adições**),\n",
        "3. Adicionar o bias \\( b \\) (**1 adição**).\n",
        "\n",
        "**Total adicional:**\n",
        "\n",
        "$$\n",
        "450 + 449 + 1 = 900 \\text{ operações}.\n",
        "$$\n",
        "\n",
        "Custo Total $ C_{\\text{original}} $\n",
        "O custo computacional total da SVM é:\n",
        "\n",
        "$$\n",
        "C_{\\text{original}} = 4050 + 900 = 4950 \\text{ operações}.\n",
        "$$\n",
        "\n",
        "**2. Custo do Perceptron $ C_{\\text{perceptron}} $**\n",
        "Ao converter a SVM linear em um perceptron, o hiperplano de decisão é dado por:\n",
        "\n",
        "$$\n",
        "f(\\mathbf{x}) = \\text{sgn}\\left( \\langle \\mathbf{w}, \\mathbf{x} \\rangle + b \\right)\n",
        "$$\n",
        "\n",
        "onde\n",
        "\n",
        "$$\n",
        "\\mathbf{w} = \\sum_{i=1}^{450} \\alpha_i y_i \\mathbf{x}_i\n",
        "$$\n",
        "\n",
        "é o vetor de pesos.\n",
        "\n",
        "Cálculo do Produto Interno $ (<w, x>) $\n",
        "- O vetor w tem dimensão $ K=5 $, assim como x.\n",
        "- O produto interno requer:\n",
        "  - **5 multiplicações** (uma para cada dimensão),\n",
        "  - **4 adições** (para somar os resultados).\n",
        "\n",
        "**Adição do Bias**\n",
        "- **1 adição** para incluir \\( b \\).\n",
        "\n",
        "**Custo Total (\\( C_{\\text{perceptron}} \\))**\n",
        "$$\n",
        "C_{\\text{perceptron}} = 5 + 4 + 1 = 10 \\text{ operações}.\n",
        "$$\n",
        "\n",
        "**3. Fator de Redução (\\( F \\))**\n",
        "$$\n",
        "F = \\frac{C_{\\text{original}}}{C_{\\text{perceptron}}} = \\frac{4950}{10} = 495.\n",
        "$$\n",
        "\n",
        "O fator de redução do custo computacional é **495**.  \n",
        "Ou seja, o perceptron é **495 vezes mais eficiente** que a SVM original no teste."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01c76af3",
      "metadata": {
        "id": "01c76af3"
      },
      "source": [
        "    4 - Interpretação do resultado de projeto de SVMs.\n",
        "    O treinamento de uma SVM linear usando a classe SVC do sklearn retornou o resultado:\n",
        "    \n",
        "    svm.n_support_= [1 2] # número de vetores de suporte por classe\n",
        "    svm.support_vectors_= [[ 1. 4.] # vetores de suporte\n",
        "                          [-2. 3.]\n",
        "                          [-2. -5.]]\n",
        "    svm.dual_coef_= [[-0.5 -0.3 0.8]] # valores de \\lambda\n",
        "    svc.intercept_= [-2] # “bias”\n",
        "    Observe que esta questão não usa o conjunto de treino indicado, mas outro. Pede-se:\n",
        "\n",
        "    a) A função de decisão para essa SVM de acordo com a fórmula geral:\n",
        "\n",
        "A função de decisão de uma SVM é dada por:  \n",
        "\n",
        "$$\n",
        "f(\\mathbf{z}) = \\left( \\sum_{n=0}^{N-1} \\lambda_n K(\\mathbf{z}, \\mathbf{x}_n) \\right) + b\n",
        "$$\n",
        "\n",
        "onde:  \n",
        "- $ \\lambda_n \\ $ são os coeficientes duais (fornecidos por dual_coef_),  \n",
        "- $ K(\\mathbf{z}, \\mathbf{x}_n) $ é o kernel linear (produto interno $ \\langle \\mathbf{z}, \\mathbf{x}_n \\rangle $).  \n",
        "\n",
        "Substituindo os valores:  \n",
        "\n",
        "$$\n",
        "f(\\mathbf{z}) = -0.5 \\cdot \\langle \\mathbf{z}, [1, 4] \\rangle - 0.3 \\cdot \\langle \\mathbf{z}, [-2, 3] \\rangle + 0.8 \\cdot \\langle \\mathbf{z}, [-2, -5] \\rangle - 2.\n",
        "$$  \n",
        "\n",
        "\n",
        "    b) A função de decisão desta SVM quando escrita como um perceptron f(z)=<z,w>+b.\n",
        "\n",
        "**b) Função de decisão como um perceptron**  \n",
        "A SVM linear pode ser reescrita como um perceptron:  \n",
        "\n",
        "$$\n",
        "f(\\mathbf{z}) = \\langle \\mathbf{z}, \\mathbf{w} \\rangle + b\n",
        "$$\n",
        "\n",
        "onde $ \\mathbf{w} = \\sum_{n=0}^{N-1} \\lambda_n y_n \\mathbf{x}_n $.  \n",
        "\n",
        "Calculando $ \\mathbf{w} $:  \n",
        "\n",
        "$$\n",
        "\\mathbf{w} = -0.5 \\cdot [1, 4] - 0.3 \\cdot [-2, 3] + 0.8 \\cdot [-2, -5].\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbf{w} = [-0.5 \\cdot 1 - 0.3 \\cdot (-2) + 0.8 \\cdot (-2), \\ -0.5 \\cdot 4 - 0.3 \\cdot 3 + 0.8 \\cdot (-5)].\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbf{w} = [-0.5 + 0.6 - 1.6, \\ -2 - 0.9 - 4] = [-1.5, \\ -6.9].\n",
        "$$  \n",
        "\n",
        "Portanto, a função do perceptron é:  \n",
        "$$\n",
        "f(\\mathbf{z}) = <z[-1.5 - 6.9]> - 2.\n",
        "$$\n",
        "\n",
        "$$\n",
        "f(\\mathbf{z}) = -1.5 z_1 - 6.9 z_2 - 2.\n",
        "$$  \n",
        "\n",
        "\n",
        "    c) A saída da função de decisão quando o vetor de entrada é z=[0, 0] e o respectivo rótulo predito y assumindo-se que y = I( f(z) > 0 ), onde I(.) é a função “indicador”, que é 1 se o argumento é verdadeiro, ou 0 em caso contrário.\n",
        "\n",
        "Substituindo $ \\mathbf{z} = [0, 0] $ na função do perceptron:  \n",
        "\n",
        "$$\n",
        "f([0, 0]) = -1.5 \\cdot 0 - 6.9 \\cdot 0 - 2 = -2.\n",
        "$$  \n",
        "\n",
        "O rótulo predito é:  \n",
        "\n",
        "$$\n",
        "y = \\mathbb{I}(f(\\mathbf{z}) > 0) = \\mathbb{I}(-2 > 0) = 0.\n",
        "$$  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a96fa67",
      "metadata": {
        "id": "2a96fa67"
      },
      "source": [
        "    5 - Interpretação do resultado de projeto de SVMs.\n",
        "    O código ak_svm_prova1.py e o conjunto de treino em dataset_train.txt foram usados para\n",
        "    gerar a figura e o log (saída de texto) abaixo. Em suma, foram treinadas 4 SVMs, duas com\n",
        "    kernel linear (SVMs 1 e 2) e outras duas com kernel não-linear (SVMs 3 e 4).\n",
        "    \n",
        "    Pergunta-se:\n",
        "    \n",
        "    a) Quais as expressões para as SVMs 3 e 4 (não-lineares) e para a SVM 2 (linear) de acordo\n",
        "    com a nomenclatura abaixo (escreva a “fórmula” da função de decisão de cada SVM\n",
        "    indicando o valor do bias “b”, etc., considerando que z e xn são vetores já normalizados (com\n",
        "    o primeiro elemento dividido por 1000).\n",
        "    \n",
        "**Formato geral da função de decisão**:\n",
        "\n",
        "$$\n",
        "f(\\mathbf{z}) = \\left( \\sum_{n=0}^{N-1} \\lambda_n K(\\mathbf{z}, \\mathbf{x}_n) \\right) + b\n",
        "$$\n",
        "\n",
        "**SVM 2 (Linear)**\n",
        "- **Kernel**: Linear $ K(\\mathbf{z}, \\mathbf{x}_n) = \\langle \\mathbf{z}, \\mathbf{x}_n \\rangle $.\n",
        "- **Vetores de suporte**:\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_0 = [0, -4], \\quad \\mathbf{x}_1 = [-1, 2], \\quad \\mathbf{x}_5 = [-2, -2]\n",
        "$$\n",
        "\n",
        "- **Coeficientes duais**:\n",
        "\n",
        "$$\n",
        "\\lambda = [-0.46, -0.28, 0.74]\n",
        "$$\n",
        "\n",
        "- **Bias**: \\( b = -1.8 \\).\n",
        "\n",
        "**Função de decisão**:\n",
        "\n",
        "$$\n",
        "f(\\mathbf{z}) = -0.46 \\cdot \\langle \\mathbf{z}, [0, -4] \\rangle - 0.28 \\cdot \\langle \\mathbf{z}, [-1, 2] \\rangle + 0.74 \\cdot \\langle \\mathbf{z}, [-2, -2] \\rangle - 1.8\n",
        "$$\n",
        "\n",
        "**SVM 3 (RBF Kernel)**\n",
        "- **Kernel**: RBF $ K(\\mathbf{z}, \\mathbf{x}_n) = \\exp(-\\gamma \\|\\mathbf{z} - \\mathbf{x}_n\\|^2) $ , com  $ \\gamma = 0.7 $.\n",
        "- **Vetores de suporte**: Todos os 6 exemplos de treino (índices 0 a 5).\n",
        "- **Coeficientes duais**:\n",
        "\n",
        "$$\n",
        "\\lambda = [-0.91, -0.91, -0.91, 0.87, 0.87, 1.0]\n",
        "$$\n",
        "\n",
        "- **Bias**: \\( b = -0.08 \\).\n",
        "\n",
        "**Função de decisão**:\n",
        "\n",
        "$$\n",
        "f(\\mathbf{z}) = \\sum_{n=0}^{5} \\lambda_n \\cdot \\exp\\left(-0.7 \\|\\mathbf{z} - \\mathbf{x}_n\\|^2\\right) - 0.08\n",
        "$$\n",
        "\n",
        "**SVM 4 (Polinomial Kernel)**\n",
        "- **Kernel**: Polinomial $ K(\\mathbf{z}, \\mathbf{x}_n) = (\\gamma \\langle \\mathbf{z}, \\mathbf{x}_n \\rangle + \\text{coef}_0)^\\text{degree} $ , com $  \\gamma = \\text{auto} ,  \\text{degree} = 3 ,  \\text{coef}_0 = 0  $.\n",
        "- **Vetores de suporte**: Índices 0, 1, 5.\n",
        "- **Coeficientes duais**:\n",
        "\n",
        "$$\n",
        "\\lambda = [-0.008, -0.03, 0.04]\n",
        "$$\n",
        "\n",
        "- **Bias**: \\( b = -1.03 \\).\n",
        "\n",
        "**Função de decisão**:\n",
        "\n",
        "$$\n",
        "f(\\mathbf{z}) = \\sum_{n \\in \\{0,1,5\\}} \\lambda_n \\cdot (\\gamma \\langle \\mathbf{z}, \\mathbf{x}_n \\rangle)^3 - 1.03731897\n",
        "$$\n",
        "\n",
        "    b) Quais as expressões para as SVMs 1 e 2 (lineares) quando escritas como perceptrons?\n",
        "\n",
        "**SVM 1 (LinearSVC)**\n",
        "- **Vetor de pesos**:\n",
        "\n",
        "$$\n",
        "\\mathbf{w} = [-0.68, -0.10]\n",
        "$$\n",
        "\n",
        "- **Bias**: \\( b = -0.99556119 \\).\n",
        "\n",
        "**Função do perceptron**:\n",
        "\n",
        "$$\n",
        "f(\\mathbf{z}) = -0.68 z_1 - 0.10 z_2 - 0.99556119\n",
        "$$\n",
        "\n",
        "**SVM 2 (SVC com Kernel Linear)**\n",
        "- **Vetor de pesos**:\n",
        "\n",
        "$$\n",
        "\\mathbf{w} = [-1.19, -0.19]\n",
        "$$\n",
        "\n",
        "- **Bias**: \\( b = -1.8 \\).\n",
        "\n",
        "**Função do perceptron**:\n",
        "\n",
        "$$\n",
        "f(\\mathbf{z}) = -1.19 z_1 - 0.19 z_2 - 1.8\n",
        "$$\n",
        "\n",
        "    \n",
        "    c) Suponha que para a SVM 2 (linear), apenas as informações abaixo fossem fornecidas.\n",
        "    Com elas, você já pôde escrever essa SVM no formato geral de uma SVM, como no item a).\n",
        "    \n",
        "1. **Calcular $ \\mathbf{w} $**:\n",
        "\n",
        "$$\n",
        "\\mathbf{w} = \\sum_{n} \\lambda_n \\mathbf{x}_n = -0.45994152 \\cdot [0, -4] - 0.27992202 \\cdot [-1, 2] + 0.73986354 \\cdot [-2, -2]\n",
        "$$\n",
        "\n",
        "Resultado:\n",
        "\n",
        "$$\n",
        "\\mathbf{w} = [-1.19, -0.19]\n",
        "$$\n",
        "\n",
        "2. **Manter o bias**: \\( b = -1.8 \\).\n",
        "\n",
        "**Economia computacional**:\n",
        "- **SVM original**: **11 operações**.\n",
        "- **Perceptron**: **3 operações**.\n",
        "- **Redução**: $ \\frac{11}{3} \\approx 3.67\\times $ mais eficiente.\n",
        "\n",
        "  \n",
        "    Explique agora com clareza quais os passos que você adotaria para converter essa SVM linear\n",
        "    em um perceptron como descrito no item b) e indique em termos do número de multiplicações\n",
        "    e adições, qual economia no custo computacional que a implementação como perceptron\n",
        "    alcança.\n",
        "    \n",
        "    svm.n_support_= [2 1]\n",
        "    svm.support_= [0 1 5]\n",
        "    svm.support_vectors_= [[ 0. -4.]\n",
        "    [-1. 2.]\n",
        "    [-2. -2.]]\n",
        "    svm.dual_coef_= [[-0.45994152 -0.27992202 0.73986354]]\n",
        "    svc.intercept_= [-1.79954513]\n",
        "    \n",
        "    d) Para as SVMs 3 e 4 (não-lineares), indique:\n",
        "    d.1) qual o número total de vetores de suporte (SVs), considerando todas as classes.\n",
        "\n",
        "- **Número total de vetores de suporte (SVs)**:\n",
        "  - **SVM 3 (RBF)**: 6 SVs (índices 0 a 5).\n",
        "  - **SVM 4 (Polinomial)**: 3 SVs (índices 0, 1, 5).\n",
        "    \n",
        "\n",
        "    d.2) Quais os índices desses SVs no conjunto de treino e os respectivos valores dos “lambdas” (coeficientes duais)\n",
        "    \n",
        "\n",
        "**SVM 3:**  \n",
        "Coeficientes duais $ \\lambda $:\n",
        "\n",
        "$$\n",
        "\\lambda = [-0.91722233, -0.91351914, -0.91300432, 0.87185969, 0.8718861, 1.0]\n",
        "$$\n",
        "\n",
        "**SVM 4:**  \n",
        "Coeficientes duais $ \\lambda\\ $:\n",
        "\n",
        "$$\n",
        "\\lambda = [-0.00887134, -0.03133903, 0.04021037]\n",
        "$$\n",
        "\n",
        "    d.3) Qual o valor do termo independente b chamado de “bias” ou “intercept_”.\n",
        "    \n",
        "\n",
        "- **Valores do bias \\( b \\)**:\n",
        "  - **SVM 3**: \\( b = -0.08676121 \\).\n",
        "  - **SVM 4**: \\( b = -1.03731897 \\).\n",
        "\n",
        "\n",
        "    e) Considerando as saídas da SVM abaixo para o conjunto de treino, em qual dos exemplos\n",
        "    de treino esta SVM está menos “confiante” (assumindo que esses números são “confidence\n",
        "    scores”) em sua decisão e qual é a classe que esta SVM prediz para este exemplo?\n",
        "    svm.decision_function(X)= [-1.00027976 -1.00027976 -0.99977173 1.00010297\n",
        "    1.00022828 0.90993821]\n",
        "\n",
        "\n",
        "**Saída da SVM 3 (RBF)**:\n",
        "\n",
        "$$\n",
        "\\text{decision\\_function}(X) = [-1.00027976, -1.00027976, -0.99977173, 1.00010297, 1.00022828, 0.90993821]\n",
        "$$\n",
        "\n",
        "- **Menor confiança**: Índice 2 (\\(-0.99977173\\)).\n",
        "- **Classe predita**: $ \\mathbb{I}(f(\\mathbf{z}) > 0) = 0 $."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "    6) Use o scikit-learn para treinar uma SVM com kernel linear, usando hiperparâmetro: C=1.\n",
        "    Neste caso use a versão normalizada dos dados, com os fatores de normalização projetados\n",
        "    com base no conjunto de treino, para que este conjunto tenha média 0 e variância 1.\n",
        "    1) Treine a SVM com o conjunto de treino indicado no arquivo dataset_train.txt.\n",
        "    2) Indique o desempenho do modelo SVM no conjunto de teste dataset_test.txt.\n",
        "    3) Converta esta SVM para um perceptron, estime o custo computacional durante a fase de\n",
        "    teste da SVM na forma original com sua versão implementada como perceptron. Indique se\n",
        "    há algum ganho computacional em termos de memória e número de operações ao se\n",
        "    converter essa SVM para um perceptron."
      ],
      "metadata": {
        "id": "MKMsHdOga_OL"
      },
      "id": "MKMsHdOga_OL"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Carregar os dados de treino e teste\n",
        "def load_data(train_path, test_path):\n",
        "    X_train = np.loadtxt(train_path, delimiter=',', usecols=(0, 1))\n",
        "    y_train = np.loadtxt(train_path, delimiter=',', usecols=2)\n",
        "    X_test = np.loadtxt(test_path, delimiter=',', usecols=(0, 1))\n",
        "    y_test = np.loadtxt(test_path, delimiter=',', usecols=2)\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "# 2. Normalizar os dados (média=0, variância=1)\n",
        "def normalize_data(X_train, X_test):\n",
        "    scaler = StandardScaler()\n",
        "    X_train_norm = scaler.fit_transform(X_train)\n",
        "    X_test_norm = scaler.transform(X_test)\n",
        "    return X_train_norm, X_test_norm, scaler\n",
        "\n",
        "# 3. Treinar SVM linear\n",
        "def train_svm(X_train, y_train, C=1):\n",
        "    svm = SVC(kernel='linear', C=C)\n",
        "    svm.fit(X_train, y_train)\n",
        "    return svm\n",
        "\n",
        "# 4. Converter SVM para perceptron\n",
        "def svm_to_perceptron(svm):\n",
        "    w = svm.coef_[0]  # Vetor de pesos (já considera lambda * y)\n",
        "    b = svm.intercept_[0]  # Bias\n",
        "    return w, b\n",
        "\n",
        "# 5. Calcular custo computacional\n",
        "def compute_cost(svm, w):\n",
        "    n_sv = sum(svm.n_support_)  # Total de vetores de suporte\n",
        "    k = X_train.shape[1]  # Dimensão dos dados (features)\n",
        "\n",
        "    # Custo da SVM original\n",
        "    svm_ops = n_sv * (k + 1)  # Produtos internos + soma ponderada\n",
        "    svm_memory = n_sv * k + n_sv + 1  # SVs + lambdas + bias\n",
        "\n",
        "    # Custo do perceptron\n",
        "    perceptron_ops = k + 1  # Produto interno + bias\n",
        "    perceptron_memory = k + 1  # w + b\n",
        "\n",
        "    return {\n",
        "        'svm_ops': svm_ops,\n",
        "        'svm_memory': svm_memory,\n",
        "        'perceptron_ops': perceptron_ops,\n",
        "        'perceptron_memory': perceptron_memory,\n",
        "        'speedup': svm_ops / perceptron_ops,\n",
        "        'memory_reduction': 1 - (perceptron_memory / svm_memory)\n",
        "    }\n",
        "\n",
        "# Pipeline principal\n",
        "if __name__ == \"__main__\":\n",
        "    # Carregar e normalizar dados\n",
        "    X_train, y_train, X_test, y_test = load_data('dataset_train.txt', 'dataset_test.txt')\n",
        "    X_train_norm, X_test_norm, scaler = normalize_data(X_train, X_test)\n",
        "\n",
        "    # Treinar SVM\n",
        "    svm = train_svm(X_train_norm, y_train, C=1)\n",
        "    print(f\"Acurácia no teste: {svm.score(X_test_norm, y_test):.4f}\")\n",
        "    print(f\"Número de vetores de suporte: {svm.n_support_}\")\n",
        "\n",
        "    # Converter para perceptron\n",
        "    w, b = svm_to_perceptron(svm)\n",
        "    print(f\"\\nPerceptron: f(z) = {w[0]:.6f} * z1 + {w[1]:.6f} * z2 + {b:.6f}\")\n",
        "\n",
        "    # Calcular custos\n",
        "    costs = compute_cost(svm, w)\n",
        "    print(\"\\nCusto computacional:\")\n",
        "    print(f\"- SVM original: {costs['svm_ops']} operações por classificação\")\n",
        "    print(f\"- Perceptron: {costs['perceptron_ops']} operações por classificação\")\n",
        "    print(f\"- Ganho de velocidade: {costs['speedup']:.1f}x\")\n",
        "    print(f\"- Memória SVM: {costs['svm_memory']} valores armazenados\")\n",
        "    print(f\"- Memória Perceptron: {costs['perceptron_memory']} valores armazenados\")\n",
        "    print(f\"- Redução de memória: {costs['memory_reduction']*100:.1f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPt9_nDefB2s",
        "outputId": "ce26a20f-453b-4734-bf70-6f20ee4f9205"
      },
      "id": "bPt9_nDefB2s",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia no teste: 1.0000\n",
            "Número de vetores de suporte: [1 2]\n",
            "\n",
            "Perceptron: f(z) = -1.117775 * z1 + -0.355624 * z2 + 0.144827\n",
            "\n",
            "Custo computacional:\n",
            "- SVM original: 9 operações por classificação\n",
            "- Perceptron: 3 operações por classificação\n",
            "- Ganho de velocidade: 3.0x\n",
            "- Memória SVM: 10 valores armazenados\n",
            "- Memória Perceptron: 3 valores armazenados\n",
            "- Redução de memória: 70.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Os resultados apresentados mostram a conversão de uma SVM linear para um perceptron e os ganhos computacionais obtidos com essa transformação. A SVM original alcançou acurácia perfeita (1.0000) no conjunto de teste, utilizando três vetores de suporte (1 da primeira classe e 2 da segunda classe). Após a conversão para perceptron, a função de decisão passou a ser expressa pela fórmula linear $ f(z) = -1.117775 \\cdot z_1 - 0.355624 \\cdot z_2 + 0.144827 $, onde \\( z_1 \\) e \\( z_2 \\) são as features do vetor de entrada e o termo constante representa o *bias*.\n",
        "\n",
        "Em termos de eficiência computacional, a conversão para perceptron trouxe benefícios significativos. A SVM original exigia 9 operações por classificação, envolvendo o cálculo de produtos internos com todos os vetores de suporte e uma soma ponderada. Já o perceptron reduz esse custo para apenas 3 operações por classificação, correspondendo a um produto interno simples entre o vetor de entrada e os pesos do perceptron, seguido da adição do *bias*. Isso representa um ganho de velocidade de 3 vezes, tornando o processo de classificação mais rápido e adequado para aplicações em tempo real ou sistemas com recursos limitados.\n",
        "\n",
        "Além da vantagem em velocidade, a redução no uso de memória também é notável. A SVM original precisava armazenar 10 valores, incluindo os vetores de suporte e seus coeficientes, enquanto o perceptron requer apenas 3 valores (os dois pesos e o *bias*), resultando em uma redução de 70% no consumo de memória. Essa otimização é especialmente útil em dispositivos com restrições de hardware, como microcontroladores ou sistemas embarcados, onde a eficiência de recursos é crítica. Em resumo, a conversão para perceptron mantém a acurácia do modelo original, mas com ganhos expressivos em desempenho e economia de memória."
      ],
      "metadata": {
        "id": "Y7gqIUIaKHOe"
      },
      "id": "Y7gqIUIaKHOe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "    7) Sobre o projeto de SVMs:\n",
        "    a) Use o scikit-learn (exemplificado no código ak_svm.py) para treinar uma SVM com kernel\n",
        "    Gaussiano (também chamado de “RBF”), usando o conjunto de validação para fazer o\n",
        "    “tuning” (otimização) de dois hiperparâmetros: “C” e “gamma”, avaliando os valores:\n",
        "    C = 0.01\n",
        "    C = 1\n",
        "    C = 100\n",
        "    gamma = 0.5\n",
        "    gamma = 1\n",
        "    Em suma:\n",
        "    1) Treine as SVMs com o conjunto de treino variando C e gamma;\n",
        "    2) Observe o resultado usando o conjunto de validação;\n",
        "    3) Indique qual SVM apresenta melhor performance no conjunto de validação.\n",
        "    Obs: O produto Cartesiano das opções de hiperparâmetros irá exigir 3 x 2 = 6 treinos de\n",
        "    SVMs.\n",
        "    b) Descreva totalmente (por extenso) os parâmetros que compõem a fórmula final do\n",
        "    classificador SVM escolhido, indicando:\n",
        "    b1) Os parâmetros escolhidos do kernel (gamma neste caso, visto que C é apenas um\n",
        "    hiperparâmetro, usado no treino mas não no teste)\n",
        "    b2) O número de vetores de suporte (SVs)\n",
        "    b3) Seus índices no conjunto de treino\n",
        "    b4) Os respectivos valores dos “lambdas” (ou “dual_coef_” de coeficientes duais) e\n",
        "    b5) O termo independente b chamado de “bias” ou “intercept_”. Em outras palavras, indique\n",
        "    os números que serão usados na função de decisão f, que pode ser escrita abaixo, onde n é o\n",
        "    índice do SV no conjunto de treino:\n",
        "    ou como na documentação do scikit-learn em https://scikit-\n",
        "    learn.org/stable/modules/svm.html#svm-mathematical-formulation, onde li = yi ai, onde yi é\n",
        "    o “label” do SV no conjunto de treino. Diferente do n no somatório acima, o índice i no\n",
        "    somatório abaixo já seleciona apenas os vetores de suporte. O somatório acima, percorre todo\n",
        "    o conjunto de treino, e os exemplos que não são vetores de suporte possuem n = 0 e daí não\n",
        "    são contabilizados.\n",
        "    Note que a documentação é confusa, pois chama de “dual coefficients” para ai e depois indica\n",
        "    que dual_coef_ armazena o produto li = yi ai.\n",
        "    c) Usando a SVM escolhida e os dois primeiros exemplos z1 e z2 do conjunto de teste\n",
        "    descrito em dataset_test.txt, compare o resultado numérico do “score” que você obtém para\n",
        "    a função de decisão f(z) calculada pelos parâmetros do item b) anterior com os gerados pela\n",
        "    função decision_function() da classe SVM do scikit-learn. Busque explicar eventuais\n",
        "    discrepâncias."
      ],
      "metadata": {
        "id": "yIJmz5hphq9b"
      },
      "id": "yIJmz5hphq9b"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import svm, datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Função para carregar o dataset personalizado\n",
        "def load_custom_dataset():\n",
        "    my_data = np.genfromtxt('dataset_train.txt', delimiter=',')\n",
        "    X = my_data[:, :2]  # Assume que as duas primeiras colunas são features\n",
        "    y = np.ravel(my_data[:, 2:])  # Assume que a terceira coluna é o target\n",
        "    return X, y\n",
        "\n",
        "# Carregar e preparar dados\n",
        "X, y = load_custom_dataset()\n",
        "\n",
        "# Dividir em treino e validação (80/20)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalização (baseada no treino)\n",
        "scaler = StandardScaler()\n",
        "X_train_norm = scaler.fit_transform(X_train)\n",
        "X_val_norm = scaler.transform(X_val)\n",
        "\n",
        "# Hiperparâmetros para tuning\n",
        "C_values = [0.01, 1, 100]\n",
        "gamma_values = [0.5, 1]\n",
        "\n",
        "# Parte a) Tuning de hiperparâmetros\n",
        "best_score = -1\n",
        "best_svm = None\n",
        "for C in C_values:\n",
        "    for gamma in gamma_values:\n",
        "        svm_rbf = svm.SVC(kernel='rbf', C=C, gamma=gamma, verbose=False)\n",
        "        svm_rbf.fit(X_train_norm, y_train)\n",
        "        score = svm_rbf.score(X_val_norm, y_val)\n",
        "        print(f\"C={C}, gamma={gamma}: Acurácia={score:.4f}\")\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_svm = svm_rbf\n",
        "\n",
        "print(\"\\nMelhor SVM:\")\n",
        "print(f\"C={best_svm.C}, gamma={best_svm.gamma}, Acurácia={best_score:.4f}\")\n",
        "\n",
        "# Parte b) Descrição detalhada do melhor modelo\n",
        "print(\"\\nParâmetros do melhor modelo:\")\n",
        "print(f\"b1) gamma = {best_svm.gamma}\")\n",
        "print(f\"b2) Número de vetores de suporte por classe: {best_svm.n_support_}\")\n",
        "print(f\"b3) Índices dos SVs no conjunto de treino: {best_svm.support_}\")\n",
        "print(f\"b4) Coeficientes duais (λ * y):\\n{best_svm.dual_coef_}\")\n",
        "print(f\"b5) Bias (intercept_): {best_svm.intercept_[0]}\")\n",
        "\n",
        "# Parte c) Comparação manual vs decision_function\n",
        "def manual_decision_function(z, svm):\n",
        "    gamma = svm.gamma\n",
        "    SVs = svm.support_vectors_\n",
        "    dual_coef = svm.dual_coef_[0]\n",
        "    b = svm.intercept_[0]\n",
        "    total = 0\n",
        "    for i in range(len(SVs)):\n",
        "        x_i = SVs[i]\n",
        "        kernel = np.exp(-gamma * np.linalg.norm(z - x_i)**2)\n",
        "        total += dual_coef[i] * kernel\n",
        "    return total + b\n",
        "\n",
        "# Carregar dados de teste\n",
        "test_data = np.genfromtxt('dataset_test.txt', delimiter=',')\n",
        "X_test = test_data[:, :2]\n",
        "y_test = np.ravel(test_data[:, 2:])\n",
        "X_test_norm = scaler.transform(X_test)\n",
        "\n",
        "# Comparar para os dois primeiros exemplos\n",
        "z1, z2 = X_test_norm[0], X_test_norm[1]\n",
        "\n",
        "f_z1_manual = manual_decision_function(z1, best_svm)\n",
        "f_z1_sklearn = best_svm.decision_function([z1])[0]\n",
        "print(f\"\\nComparação para z1:\")\n",
        "print(f\"Manual: {f_z1_manual:.6f}, Scikit-learn: {f_z1_sklearn:.6f}\")\n",
        "\n",
        "f_z2_manual = manual_decision_function(z2, best_svm)\n",
        "f_z2_sklearn = best_svm.decision_function([z2])[0]\n",
        "print(f\"\\nComparação para z2:\")\n",
        "print(f\"Manual: {f_z2_manual:.6f}, Scikit-learn: {f_z2_sklearn:.6f}\")"
      ],
      "metadata": {
        "id": "9fy5-q71jHBz",
        "outputId": "f31a453e-f392-418a-8823-513d0fe0ce1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "9fy5-q71jHBz",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C=0.01, gamma=0.5: Acurácia=0.0000\n",
            "C=0.01, gamma=1: Acurácia=0.0000\n",
            "C=1, gamma=0.5: Acurácia=1.0000\n",
            "C=1, gamma=1: Acurácia=1.0000\n",
            "C=100, gamma=0.5: Acurácia=1.0000\n",
            "C=100, gamma=1: Acurácia=1.0000\n",
            "\n",
            "Melhor SVM:\n",
            "C=1, gamma=0.5, Acurácia=1.0000\n",
            "\n",
            "Parâmetros do melhor modelo:\n",
            "b1) gamma = 0.5\n",
            "b2) Número de vetores de suporte por classe: [1 3]\n",
            "b3) Índices dos SVs no conjunto de treino: [2 0 1 3]\n",
            "b4) Coeficientes duais (λ * y):\n",
            "[[-1.          0.44858894  0.44544123  0.10596983]]\n",
            "b5) Bias (intercept_): 0.5513468251219731\n",
            "\n",
            "Comparação para z1:\n",
            "Manual: -0.303535, Scikit-learn: -0.303535\n",
            "\n",
            "Comparação para z2:\n",
            "Manual: 0.897922, Scikit-learn: 0.897922\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Os resultados demonstram o processo de otimização de hiperparâmetros para uma SVM com kernel RBF, onde foram testadas combinações de valores para C (0.01, 1, 100) e gamma (0.5, 1). Observamos que:\n",
        "\n",
        "**1. Desempenho por Configuração:**\n",
        "- Para C=0.01 (com ambos gammas), a acurácia foi 0%, indicando que o modelo foi excessivamente regularizado e não conseguiu aprender padrões úteis nos dados.\n",
        "- Todas as outras combinações (C=1 e C=100) alcançaram acurácia perfeita (100%), mostrando que esses valores permitiram um melhor ajuste aos dados.\n",
        "\n",
        "**2. Melhor Modelo Selecionado:**\n",
        "O modelo com C=1 e gamma=0.5 foi escolhido como o melhor, embora outras combinações tenham tido o mesmo desempenho. Essa escolha pode ser justificada por:\n",
        "- Evitar overfitting (C=1 é menos propenso a sobreajuste que C=100)\n",
        "- Manter boa generalização (gamma=0.5 oferece um equilíbrio na influência dos vetores de suporte)\n",
        "\n",
        "**3. Características do Modelo Final:**\n",
        "- Utiliza 4 vetores de suporte (1 da primeira classe e 3 da segunda)\n",
        "- Os coeficientes duais mostram que o primeiro vetor de suporte tem maior peso (λ=-1.0)\n",
        "- O bias positivo (0.551) indica um limiar de decisão deslocado\n",
        "\n",
        "**4. Validação da Função de Decisão:**\n",
        "A comparação entre os cálculos manuais e a função decision_function() do scikit-learn mostrou concordância perfeita para dois exemplos de teste:\n",
        "- z1 foi classificado como negativo com score -0.303535\n",
        "- z2 foi classificado como positivo com score 0.897922\n",
        "\n",
        "**5. Implicações Práticas:**\n",
        "- O modelo final tem excelente capacidade de generalização (acurácia 100%)\n",
        "- A consistência nos cálculos valida a implementação\n",
        "- A seleção de C=1 e gamma=0.5 sugere que:\n",
        "  * Um C moderado foi suficiente para separar as classes\n",
        "  * Um gamma intermediário capturou bem a complexidade dos dados sem sobreajuste\n"
      ],
      "metadata": {
        "id": "FSPrVFE5NhD_"
      },
      "id": "FSPrVFE5NhD_"
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}